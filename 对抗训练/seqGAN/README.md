## 基础知识巩固

### 转载
原文地址：https://zhuanlan.zhihu.com/p/2916880

文本生成按任务来说，比较流行的有：机器翻译、句子生成、对话生成等，本文着重讨论后面两种。基于深度学习的Text Generator 通常使用循环神经网络（Basic RNN，LSTM，GRU等）进行语义建模。在句子生成任务中，一种常见的应用：“Char-RNN”（这里“Char”是广义上的称谓，可以泛指一个字符、单词或其他文本粒度单位），虽然简单基础但可以清晰度反应句子生成的运行流程，首先需要建立一个词库Vocab包含可能出现的所有字符或是词汇，每次模型将预测得到句子中下一个将出现的词汇，要知道softmax输出的只是一个概率分布，其维度为词库 Vocab 的size，需再通过函数将输出概率分布转化为 One-hot vector，从词库 Vocab中检索得出对应的词项；在“Char-RNN”模型训练时，使用窗口在语料上滑动，窗口之内的上下文及其后紧跟的字符配合分别为一组训练样本和标签，每次以按照固定的步长滑动窗口以得出全部 “样本-标签” 对。

与句子生成任务类似，对话生成以每组Dialogue作为 “样本-标签” 对，循环神经网络RNN_1对Dialogue上文进行编码，再用另一个循环神经网络RNN_2对其进行逐词解码，并以上一个解码神经元的输出作为下一个解码神经元的输入，生成Dialogue下文，需要注意的是：在解码前需配置“开始”标记 _，用于指示解码器Decoder开启Dialogue下文首词（or 字）的生成，并配置“结束”标记 _，用于指示解码器结束当前的 Text Generation 进程。

这便是众所周知的“Seq2Seq”框架的基础形态，为了提高基础Seq2Seq模型的效果，直接从解码器的角度有诸如 Beam-SearchDecoder[2]、Attention mechanism Decoder[3]（配置注意力机制的解码器）等改进，而从神经网络的结构入手，也有诸如Pyramidal RNN[4]（金字塔型RNN）、Hierarchical RNN Encoder[5]（分层循环网络编码器）等改进。改进不计其数，不一一详举，但不管如何，预测结果的输出始终都是一个维度为词库大小的概率分布，需要再甄选出最大值的Index，到词库Vocab中检索得出对应的单词（or 字符）。






























